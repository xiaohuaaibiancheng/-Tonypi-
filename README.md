

# Tonypi 多模态机器人项目

本项目旨在赋予 Tonypi 机器人更强大的多模态交互能力，特别是在复杂指令理解、场景认知与行为模仿方面。通过丰富的模块设计，实现了从低层动作到高层智能的全面控制。

## 目录结构

```
├── ActionGroups/     # 预设动作文件，控制机器人执行具体动作
├── Functions/        # 机器人核心功能实现
├── models/ 存放视觉识别相关模型文件       
├── command.py        # 多模态扩展功能的主入口
├── Transport_to_words.py #利用语音转文字api 转文字的脚本
├── tts_ws_python3_demo.py # 文本转音频的脚本
```

## 主要功能介绍

### 1. ActionGroups 动作组

- 存放机器人的动作文件，每个文件对应一组动作序列。
- 执行相应动作组文件后，机器人将完成特定动作（如挥手、踢球、搬运物体等）。

### 2. Functions 文件夹

包含各种控制机器人行为的脚本和模块，包括：

1. **视觉颜色识别**：通过摄像头识别不同颜色目标。
2. **简单动作控制**：如前进、后退、转向等基础动作操作。
3. **指定位置物体搬运**：机器人可自主寻路至目标点并搬运物体。
4. **定线巡航**：沿预设路线自动巡航。
5. **人脸追踪**：自动识别人脸并持续跟随。
6. **物体跟踪**：对指定物体进行跟踪。
7. **跟踪踢球**：识别并追踪球体，完成踢球动作。

### 3. command.py 多模态扩展

实现了更高层次的多模态智能能力，包括：

1. **场景描述**：分析当前环境，生成自然语言的场景描述。
2. **复制指令执行**：通过模仿演示动作，实现指令复制与动作复现。
3. **智能对话（带长记忆）**：支持与用户进行上下文相关的多轮对话，并具备长时记忆能力。
4. **行为模仿**：学习和复现给定的动作序列，实现人类动作模仿。


#### 3.1主要功能

- **语音识别与合成**  
  支持普通话口语指令、命令词唤醒、连续自然对话，机器人可用语音自然回应。

- **动作规划与执行**  
  利用阿里云通义千问大模型，将中文自然语言指令解析为动作函数序列，自动控制机器人运动、表演等。

- **场景视觉理解**  
  调用摄像头拍照，结合大模型多模态能力，输出诗意场景描述，并通过语音播报。

- **动作模仿**  
  AI 识别图片中的人物动作，自动生成机器人舵机参数，实现模仿表演。

- **环顾四周**  
  一键控制机器人头部舵机，完成“环顾”动作，并有语音提示。

## 快速开始

1. **环境准备**
   - 树莓派（或等效 Linux 环境）
   - Python 3.7+
   - 依赖库：`opencv-python`, `numpy`, `requests`, `websocket-client` 等
   - 机器人硬件驱动（见 `hiwonder/` 目录）

2. **主要文件说明**
   - `command.py`：主程序，包含全部核心功能
   - `hiwonder/`：硬件及语音模块驱动
   - `tts_ws_python3_demo.py`：科大讯飞 TTS 语音合成示例
   - `Transport_to_words.py`：录音辅助

3. **启动方式**

   ```bash
   python3 command.py

## 快速开始

1. 克隆仓库：
   ```bash
   git clone https://github.com/xiaohuaaibiancheng/-Tonypi-.git
   ```
2. 按照 `Functions/` 目录下的说明，部署相应依赖和环境。
3. 运行 `command.py`，体验多模态交互与智能行为功能。

## 贡献与交流

欢迎对本项目提出建议和贡献代码。  
如有问题请通过 Issues 或 Discussions 进行反馈。

